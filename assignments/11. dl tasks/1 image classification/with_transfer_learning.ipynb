{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67450816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b155aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"C:/Users/Yakina/.cache/kagglehub/datasets/mdwaquarazam/agricultural-crops-image-classification/versions/1/Agricultural-crops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c384c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e1f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8796c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transform)\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e33d627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cherry',\n",
       " 'Coffee-plant',\n",
       " 'Cucumber',\n",
       " 'Fox_nut(Makhana)',\n",
       " 'Lemon',\n",
       " 'Olive-tree',\n",
       " 'Pearl_millet(bajra)',\n",
       " 'Tobacco-plant',\n",
       " 'almond',\n",
       " 'banana',\n",
       " 'cardamom',\n",
       " 'chilli',\n",
       " 'clove',\n",
       " 'coconut',\n",
       " 'cotton',\n",
       " 'gram',\n",
       " 'jowar',\n",
       " 'jute',\n",
       " 'maize',\n",
       " 'mustard-oil',\n",
       " 'papaya',\n",
       " 'pineapple',\n",
       " 'rice',\n",
       " 'soyabean',\n",
       " 'sugarcane',\n",
       " 'sunflower',\n",
       " 'tea',\n",
       " 'tomato',\n",
       " 'vigna-radiati(Mung)',\n",
       " 'wheat']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d07246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027e1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply validation transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc78cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecec373",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee594907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yakina\\anaconda3\\envs\\traineeship\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yakina\\anaconda3\\envs\\traineeship\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze pretrained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21491902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace final FC layer\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06751a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e92dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8b35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.3687, Accuracy: 0.0784\n",
      "Epoch 2/50, Loss: 2.5763, Accuracy: 0.3786\n",
      "Epoch 3/50, Loss: 2.0181, Accuracy: 0.6078\n",
      "Epoch 4/50, Loss: 1.6259, Accuracy: 0.7164\n",
      "Epoch 5/50, Loss: 1.3263, Accuracy: 0.7828\n",
      "Epoch 6/50, Loss: 1.1257, Accuracy: 0.8326\n",
      "Epoch 7/50, Loss: 0.9770, Accuracy: 0.8582\n",
      "Epoch 8/50, Loss: 0.8490, Accuracy: 0.8854\n",
      "Epoch 9/50, Loss: 0.7621, Accuracy: 0.8959\n",
      "Epoch 10/50, Loss: 0.6840, Accuracy: 0.9186\n",
      "Epoch 11/50, Loss: 0.5958, Accuracy: 0.9276\n",
      "Epoch 12/50, Loss: 0.5550, Accuracy: 0.9442\n",
      "Epoch 13/50, Loss: 0.5038, Accuracy: 0.9517\n",
      "Epoch 14/50, Loss: 0.4706, Accuracy: 0.9472\n",
      "Epoch 15/50, Loss: 0.4255, Accuracy: 0.9487\n",
      "Epoch 16/50, Loss: 0.3844, Accuracy: 0.9668\n",
      "Epoch 17/50, Loss: 0.3550, Accuracy: 0.9804\n",
      "Epoch 18/50, Loss: 0.3458, Accuracy: 0.9729\n",
      "Epoch 19/50, Loss: 0.3171, Accuracy: 0.9804\n",
      "Epoch 20/50, Loss: 0.2818, Accuracy: 0.9864\n",
      "Epoch 21/50, Loss: 0.2651, Accuracy: 0.9819\n",
      "Epoch 22/50, Loss: 0.2633, Accuracy: 0.9894\n",
      "Epoch 23/50, Loss: 0.2433, Accuracy: 0.9910\n",
      "Epoch 24/50, Loss: 0.2284, Accuracy: 0.9925\n",
      "Epoch 25/50, Loss: 0.2086, Accuracy: 0.9925\n",
      "Epoch 26/50, Loss: 0.1965, Accuracy: 0.9970\n",
      "Epoch 27/50, Loss: 0.1893, Accuracy: 0.9925\n",
      "Epoch 28/50, Loss: 0.1841, Accuracy: 0.9985\n",
      "Epoch 29/50, Loss: 0.1667, Accuracy: 0.9910\n",
      "Epoch 30/50, Loss: 0.1597, Accuracy: 0.9985\n",
      "Epoch 31/50, Loss: 0.1487, Accuracy: 0.9955\n",
      "Epoch 32/50, Loss: 0.1460, Accuracy: 1.0000\n",
      "Epoch 33/50, Loss: 0.1403, Accuracy: 0.9955\n",
      "Epoch 34/50, Loss: 0.1452, Accuracy: 0.9955\n",
      "Epoch 35/50, Loss: 0.1261, Accuracy: 0.9985\n",
      "Epoch 36/50, Loss: 0.1207, Accuracy: 0.9985\n",
      "Epoch 37/50, Loss: 0.1197, Accuracy: 0.9985\n",
      "Epoch 38/50, Loss: 0.1175, Accuracy: 1.0000\n",
      "Epoch 39/50, Loss: 0.1168, Accuracy: 0.9985\n",
      "Epoch 40/50, Loss: 0.1028, Accuracy: 0.9985\n",
      "Epoch 41/50, Loss: 0.1026, Accuracy: 0.9985\n",
      "Epoch 42/50, Loss: 0.0952, Accuracy: 1.0000\n",
      "Epoch 43/50, Loss: 0.1019, Accuracy: 0.9955\n",
      "Epoch 44/50, Loss: 0.0902, Accuracy: 1.0000\n",
      "Epoch 45/50, Loss: 0.0896, Accuracy: 1.0000\n",
      "Epoch 46/50, Loss: 0.0866, Accuracy: 1.0000\n",
      "Epoch 47/50, Loss: 0.0802, Accuracy: 1.0000\n",
      "Epoch 48/50, Loss: 0.0852, Accuracy: 1.0000\n",
      "Epoch 49/50, Loss: 0.0833, Accuracy: 1.0000\n",
      "Epoch 50/50, Loss: 0.0781, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85591555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4093962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             Cherry       0.83      0.83      0.83         6\n",
      "       Coffee-plant       1.00      0.83      0.91         6\n",
      "           Cucumber       0.83      1.00      0.91         5\n",
      "   Fox_nut(Makhana)       0.62      0.83      0.71         6\n",
      "              Lemon       0.75      0.50      0.60         6\n",
      "         Olive-tree       0.40      1.00      0.57         4\n",
      "Pearl_millet(bajra)       0.80      1.00      0.89         8\n",
      "      Tobacco-plant       1.00      0.56      0.71         9\n",
      "             almond       1.00      0.25      0.40         4\n",
      "             banana       0.62      0.71      0.67         7\n",
      "           cardamom       1.00      0.43      0.60         7\n",
      "             chilli       0.50      0.67      0.57         3\n",
      "              clove       0.00      0.00      0.00         0\n",
      "            coconut       1.00      1.00      1.00         5\n",
      "             cotton       0.69      1.00      0.82         9\n",
      "               gram       0.75      0.60      0.67         5\n",
      "              jowar       1.00      0.67      0.80         9\n",
      "               jute       0.60      0.75      0.67         4\n",
      "              maize       0.50      0.80      0.62         5\n",
      "        mustard-oil       0.60      0.60      0.60         5\n",
      "             papaya       1.00      0.83      0.91         6\n",
      "          pineapple       1.00      0.83      0.91         6\n",
      "               rice       0.80      1.00      0.89         4\n",
      "           soyabean       0.75      0.86      0.80         7\n",
      "          sugarcane       1.00      0.60      0.75         5\n",
      "          sunflower       0.80      0.50      0.62         8\n",
      "                tea       0.80      1.00      0.89         4\n",
      "             tomato       0.75      1.00      0.86         3\n",
      "vigna-radiati(Mung)       0.00      0.00      0.00         6\n",
      "              wheat       0.80      1.00      0.89         4\n",
      "\n",
      "           accuracy                           0.73       166\n",
      "          macro avg       0.74      0.72      0.70       166\n",
      "       weighted avg       0.78      0.73      0.73       166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yakina\\anaconda3\\envs\\traineeship\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Yakina\\anaconda3\\envs\\traineeship\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Yakina\\anaconda3\\envs\\traineeship\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f81af72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'img_classification_w_tl.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traineeship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
